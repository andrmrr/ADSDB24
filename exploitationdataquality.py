# -*- coding: utf-8 -*-
"""ExploitationDataQuality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lonGJ4rkSFW3i1wp6Flf4pyQs51L8Fhb
"""

from google.colab import drive
drive.mount('/content/drive')

pip install pyspellchecker

!pip install --quiet duckdb-engine
!pip install --quiet pandas

"""# Exploitation zone


"""

import duckdb
import pandas as pd
import time
import os

from spellchecker import SpellChecker

base_dir = "/content/drive/MyDrive/ADSDB24"
temporal_landing = os.path.join(base_dir,  "landing_zone/temporal")
persistent_landing = os.path.join(base_dir, "landing_zone/persistent")
duckdb_db_trusted = os.path.join(base_dir, "duckdb_database", "trusted_zone_test.db")
duckdb_db_formatted = os.path.join(base_dir, "duckdb_database", "formatted_zone.db")
duckdb_db_formatted = os.path.join(base_dir, "duckdb_database", "exploitation_zone.db")

datasource1 = os.path.join(base_dir, "datasets_original/Alzheimer_s_Disease_and_Healthy_Aging_Data.csv")
datasource1_name = "alzheimer"
datasource2 = os.path.join(base_dir, "datasets_original/U.S._Chronic_Disease_Indicators__CDI___2023_Release.csv")
datasource2_name = "chronic_disease_indicators"

datasource1tmp = os.path.join(base_dir, "tmp_dir/Air_Quality.csv")
datasource2tmp = os.path.join(base_dir, "tmp_dir/Death_rates_for_suicide__by_sex__race__Hispanic_origin__and_age__United_States.csv")

datasources = {
    "Air_Quality.csv": datasource1_name,
    "Death_rates_for_suicide__by_sex__race__Hispanic_origin__and_age__United_States.csv": datasource2_name,
}

"""Temporary load mock data to table"""

# Function to get the latest dataset file in the folder
def get_latest_dataset(folder):
    files = [f for f in os.listdir(folder) if f.endswith('.csv')]
    if files:
      # Get the newest file by creation time
        latest_file = max(files, key=lambda f: os.path.getctime(os.path.join(folder, f)))
        return latest_file
    else:
        return None

# Function for adding new dataset if there is a new one
def add_new_dataset(ds):
  with duckdb.connect(duckdb_db_trusted) as con:

      curr_timestamp = time.strftime("%d%m%Y_%H")
      # Parse the dataset name (remove the .csv extension)
      dataset_name = datasources[ds]

      # Create new table name by adding the timestamp to dataset name
      table_name = f"{dataset_name}_{curr_timestamp}"

      # dataset_path = os.path.join(persistent_landing, dataset_name, ds)
      dataset_path = os.path.join(base_dir, "tmp_dir", ds)
      df = pd.read_csv(dataset_path)

      # Create a new table in database
      con.sql(f"CREATE TABLE {table_name} AS SELECT * FROM df")
      print(f"Table '{table_name}' has been created in the database.")

add_new_dataset("Air_Quality.csv")

"""Checking for new datasets in persistent landing zone and updating them to the database"""

with duckdb.connect(duckdb_db_formatted) as con:
  con.sql("SHOW ALL TABLES").show()

"""Data ingestion"""

# TODO Proveri da li ima novih tabela u formatted zone
def load_trusted(table):
  with duckdb.connect(duckdb_db_formatted) as con:
    df = con.sql("SELECT * FROM " + table).df()
    # print(df.info())
    # con.sql("SELECT * FROM alzheimer_16102024_18").show()
    return df



alz = load_trusted("alzheimer")
chr = load_trusted("chronic_diseases")

"""Data quality processes"""

alz.info()

def profiling(df):
  # print(df[:4])
  print(df.shape)
  print(df.columns)
  print(df.describe())
  print(df.describe(include='object'))

"""# Alzheimer dataset"""

profiling(alz)

print(alz.columns)

alz_metadata = {}
for col in alz.columns:
  unique_count = len(alz[col].unique())
  if unique_count <= 1:
    print(f"{col}: {unique_count}")
    print(f"---> Value: {alz[col][0]}")
    alz_metadata[col] = alz[col][0]
# print(alz['RowId'].unique())
print(alz_metadata)

alz_modified = alz.drop(["LocationAbbr", "LocationDesc", "Datasource", "Geolocation", "LocationID"], axis=1)
print(alz_modified.columns)

"""We get the same value in all rows for the following columns with following values: LocationAbbr - TX, LocationDesc - Texas, Datasource - BRFSS, StratificationCategory1 - Age Group, Geolocation - POINT(-99.42677021 31.82724041), LocationID - 48 and StratificationCategoryID1 - AGE.

Location related features are chosen in advance, so we can take them out and save the static values, which hold true for the entire dataset. Datasource will also be moved to metadata and removed from the dataset.

We will leave the stratification features in for now.
"""

# rename the first column
alz_modified.rename(columns={"Unnamed: 0": "RowNumber"}, inplace=True)

# actually drop it
alz_modified.drop("RowNumber", axis=1, inplace=True)
alz_modified.columns

for col in alz_modified.columns:
  has_null = alz_modified[col].isnull().any()
  if has_null:
    print(f"{col}: {alz_modified[col].isnull().sum()}")

"""Since Data_Value and Data_Value_Alt are identical, we drop Data_Value_Alt"""

# check if Data_Value and Data_Value_Alt are identical, and if so remove Data_Value_Alt
if (alz_modified["Data_Value"] == alz_modified["Data_Value_Alt"]).all():
  alz_modified.drop("Data_Value_Alt", axis=1, inplace=True)

"""Maybe we should combine YearStart and YearEnd into a single column"""

print(alz_modified["Data_Value_Footnote_Symbol"].unique())
print(alz_modified["Data_Value_Footnote"].unique())

# drop Data_Value_Footnote_Symbol and Data_Value_Footnote columns
alz_modified.drop(["Data_Value_Footnote_Symbol", "Data_Value_Footnote"], axis=1, inplace=True)

print(alz_modified["Data_Value_Unit"].unique())
print(alz_modified["DataValueTypeID"].unique())
print(alz_modified["Data_Value_Type"].unique())

# drop Data_Value_Unit, DataValueTypeID and Data_Value_Type columns
alz_modified.drop(["Data_Value_Unit", "DataValueTypeID", "Data_Value_Type"], axis=1, inplace=True)

"""I feel that no normalization/scaling is necessary for this dataset"""

profiling(alz_modified)

"""# Chronic diseases dataset"""

chr.info()

profiling(chr)

for col in chr_modified.columns:
  has_null = chr_modified[col].isnull().any()
  if has_null:
    print(f"{col}: {chr_modified[col].isnull().sum()}")

print(chr_modified["DataValueFootnoteSymbol"].unique())
print(chr_modified["DatavalueFootnote"].unique())

chr_modified.drop(["DataValueFootnoteSymbol", "DatavalueFootnote"], axis=1, inplace=True)

chr_metadata = {}
for col in chr_modified.columns:
  unique_count = len(chr_modified[col].unique())
  if unique_count <= 1:
    print(f"{col}: {unique_count}")
    print(f"---> Value: {chr_modified[col][0]}")
    chr_metadata[col] = chr_modified[col][0]
# print(chr['RowId'].unique())
print(chr_metadata)

chr_modified.drop(["LocationAbbr", "LocationDesc", "GeoLocation", "LocationID"], axis=1, inplace=True)

chr_modified.drop("Unnamed: 0", axis=1, inplace=True)

print(chr_modified.columns)

# print(len(chr_modified["DataValue"].unique()))
print(chr_modified[chr_modified["DataValue"] == "No"])

# check if Data_Value and Data_Value_Alt are identical, and if so remove Data_Value_Alt
print((chr_modified["DataValue"] == chr_modified["DataValueAlt"]).all())
for i, row in chr_modified.iterrows():
  dv = row["DataValue"]
  dva = row["DataValueAlt"]
  if dv != dva:
    print(f"DataValue: {dv} ------ DataValueAlt: {dva}")

chr_modified.drop("DataValueAlt", axis=1, inplace=True)

# impute missing values for Data_Value, Data_Value_Alt, Low_Confidence_Limit and High_Confidence_Limit with -1
chr_modified["LowConfidenceLimit"].fillna(-1, inplace=True)
chr_modified["HighConfidenceLimit"].fillna(-1, inplace=True)

# convert numerical to categorical
chr_modified["YearStart"] = chr_modified["YearStart"].astype('category')
chr_modified["YearEnd"] = chr_modified["YearEnd"].astype('category')

# check for duplicates
print(chr_modified.shape[0])
chr_modified.drop_duplicates(inplace=True)
print(chr_modified.shape[0])

print(chr_modified["DataValueUnit"].unique())
print(chr_modified["DataValueTypeID"].unique())

chr_modified.drop(["DataValueUnit", "DataValueTypeID"], axis=1, inplace=True)

profiling(chr_modified)